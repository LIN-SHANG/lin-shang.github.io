---
title: CS336 lecture-01 - tokenizer
linktitle: Section-01_lecture-01
summary: åŸºäº CS336 è¯¾ç¨‹è®²ä¹‰ï¼Œæ·±å…¥æ¢è®¨äº†å¤§æ¨¡å‹ä¸­çš„åˆ†è¯ï¼ˆTokenizationï¼‰æŠ€æœ¯ã€‚ä»æœ€æœ´ç´ çš„å­—ç¬¦çº§ã€å­—èŠ‚çº§åˆ†è¯ï¼Œä¸€ç›´è®²è§£åˆ°ç°ä»£ LLM æ ‡é…çš„ BPEï¼ˆByte Pair Encodingï¼‰ç®—æ³•åŠå…¶å®ç°ã€‚
date: 2025-12-16
python: true
type: docs
content_meta:
  content_type: Course
  difficulty: M.S. level and upper
  prerequisites:
    - Markdown
  trending: false
menu:
  Stanford-CS-336:
    name: Overview
    weight: 1
---

> **è‡´è°¢**ï¼šæœ¬å•å…ƒå†…å®¹çš„çµæ„Ÿæ¥æºäº Andrej Karpathy å…³äº Tokenization çš„[è§†é¢‘](https://www.youtube.com/watch?v=zduSFxRajkE)ï¼Œå¼ºçƒˆæ¨èè§‚çœ‹ã€‚

åœ¨è¯­è¨€æ¨¡å‹ä¸­ï¼Œ**Tokenizerï¼ˆåˆ†è¯å™¨ï¼‰** æ‰®æ¼”ç€â€œç¿»è¯‘å®˜â€çš„è§’è‰²ï¼Œè´Ÿè´£åœ¨ **å­—ç¬¦ä¸²ï¼ˆStringsï¼‰** å’Œ **Token åºåˆ—ï¼ˆIndicesï¼‰** ä¹‹é—´è¿›è¡Œè½¬æ¢ã€‚ è¯­è¨€æ¨¡å‹æœ¬è´¨ä¸Šæ˜¯å¯¹ Token åºåˆ—ï¼ˆé€šå¸¸ç”±æ•´æ•°ç´¢å¼•è¡¨ç¤ºï¼‰çš„æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ã€‚ 
ä¾‹å¦‚ï¼š 
- è¾“å…¥å­—ç¬¦ä¸²ï¼š`"Hello, ğŸŒ! ä½ å¥½!"` 
- Token åºåˆ—ï¼š`[15496, 11, 995, 0]` 

æˆ‘ä»¬éœ€è¦ä¸€ä¸ªè¿‡ç¨‹å°†å­—ç¬¦ä¸²ç¼–ç ï¼ˆEncodeï¼‰ä¸º Tokenï¼Œä¹Ÿéœ€è¦ä¸€ä¸ªè¿‡ç¨‹å°† Token è§£ç ï¼ˆDecodeï¼‰å›å­—ç¬¦ä¸²ã€‚è™½ç„¶å°†æ–‡æœ¬è§†ä¸º Unicode å­—ç¬¦åºåˆ—æˆ–å­—èŠ‚åºåˆ—åœ¨ç†è®ºä¸Šæ˜¯å¯è¡Œçš„ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒåŸºäº **BPEï¼ˆå­—èŠ‚å¯¹ç¼–ç ï¼‰** çš„æ–¹æ³•æ˜¯ç›®å‰æœ€é«˜æ•ˆçš„å¯å‘å¼ç®—æ³•ã€‚

ä»¥ä¸‹æˆ‘ä»¬å°†é€æ­¥æ¼”è¿›ï¼Œä»æœ€ç®€å•çš„åˆ†è¯æ–¹æ³•ç›´åˆ° BPEã€‚

## 1. åŸºç¡€åˆ†è¯æ–¹æ³•çš„å°è¯•ä¸å±€é™ 
ä¸ºäº†ç›´è§‚æ„Ÿå—åˆ†è¯å™¨çš„å·¥ä½œåŸç†ï¼Œæˆ‘ä»¬å¯ä»¥å…ˆå®šä¹‰ä¸€ä¸ªæŠ½è±¡åŸºç±»ï¼Œå¹¶å°è¯•å‡ ç§æœ´ç´ çš„å®ç°ã€‚ 

```python
from abc import ABC, abstractmethod 
class Tokenizer(ABC): 
	@abstractmethod 
	def encode(self, string: str) -> list[int]: 
		pass 
		
	@abstractmethod 
	def decode(self, indices: list[int]) -> str: 
		pass


class BPETokenizerParams:
	"""All you need to specify a BPETokenizer."""
	vocab: dict[int, bytes] # index -> bytes
	merges: dict[tuple[int, int], int] # index1, index2 -> new_index
```

### 1.1 å­—ç¬¦çº§åˆ†è¯ (Character-based Tokenization)
Unicode å­—ç¬¦ä¸²æœ¬è´¨ä¸Šæ˜¯å­—ç¬¦çš„åºåˆ—ã€‚æˆ‘ä»¬å¯ä»¥ç›´æ¥åˆ©ç”¨`ord()`Â å°†å­—ç¬¦è½¬æ¢ä¸ºç ç‚¹ï¼ˆæ•´æ•°ï¼‰ï¼Œç”¨Â `chr()`Â è½¬å›å­—ç¬¦ã€‚

```python
class CharacterTokenizer(Tokenizer):
    """å°†å­—ç¬¦ä¸²è¡¨ç¤ºä¸º Unicode ç ç‚¹åºåˆ—"""
    def encode(self, string: str) -> list[int]:
        return list(map(ord, string))

    def decode(self, indices: list[int]) -> str:
        return "".join(map(chr, indices))

# æµ‹è¯•
# string = "Hello, ğŸŒ! ä½ å¥½!"
# ord("a") == 97, ord("ğŸŒ") == 127757
```

**å±€é™æ€§ï¼š**
1. **è¯è¡¨è¿‡å¤§**ï¼šUnicode å­—ç¬¦é›†å¤§çº¦æœ‰ **1ç™¾ä¸‡**ä¸ªå­—ç¬¦ï¼Œè¿™æ„å‘³ç€æ¨¡å‹çš„è¯æ±‡è¡¨ï¼ˆVocabulary Sizeï¼‰éå¸¸åºå¤§ã€‚
2. **æ•ˆç‡ä½ä¸‹**ï¼šè®¸å¤šå­—ç¬¦ï¼ˆå¦‚Â ğŸŒï¼‰éå¸¸ç½•è§ï¼Œå æ®äº†è¯è¡¨ç©ºé—´å´å¾ˆå°‘è¢«å­¦ä¹ åˆ°ï¼Œè¿™æ˜¯å¯¹èµ„æºçš„æµªè´¹ã€‚

### 1.2 å­—èŠ‚çº§åˆ†è¯ (Byte-based Tokenization)
ä¸ºäº†è§£å†³è¯è¡¨è¿‡å¤§çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥å°† Unicode å­—ç¬¦ä¸²è§†ä¸ºå­—èŠ‚åºåˆ—ï¼ˆUTF-8 ç¼–ç ï¼‰ã€‚æ¯ä¸ªå­—èŠ‚ç”± 0 åˆ° 255 ä¹‹é—´çš„æ•´æ•°è¡¨ç¤ºã€‚

```python
class ByteTokenizer(Tokenizer):
    """å°†å­—ç¬¦ä¸²è¡¨ç¤ºä¸ºå­—èŠ‚åºåˆ—"""
    def encode(self, string: str) -> list[int]:
        string_bytes = string.encode("utf-8")
        indices = list(map(int, string_bytes))
        return indices

    def decode(self, indices: list[int]) -> str:
        string_bytes = bytes(indices)
        string = string_bytes.decode("utf-8")
        return string

# æµ‹è¯•
# bytes("a", encoding="utf-8") == b"a"
# bytes("ğŸŒ", encoding="utf-8") == b"\xf0\x9f\x8c\x8d" (4ä¸ªå­—èŠ‚)
```
**å±€é™æ€§ï¼š**  
è™½ç„¶è¯è¡¨å¤§å°å®Œç¾åœ°æ§åˆ¶åœ¨ 256ï¼Œä½†**å‹ç¼©ç‡ï¼ˆCompression Ratioï¼‰**Â æå·®ã€‚
- `Compression Ratio = Bytes / Tokens`åœ¨è¿™é‡Œï¼Œæ¯”ç‡ä¸º 1ã€‚
- è¿™æ„å‘³ç€åºåˆ—ä¼šå˜å¾—éå¸¸é•¿ã€‚ç”±äº Transformer çš„æ³¨æ„åŠ›æœºåˆ¶è®¡ç®—å¤æ‚åº¦æ˜¯åºåˆ—é•¿åº¦çš„å¹³æ–¹çº§ï¼Œè¿‡é•¿çš„åºåˆ—ä¼šå¯¼è‡´æ¨ç†æå…¶ç¼“æ…¢ä¸”ä¸Šä¸‹æ–‡çª—å£è¢«æµªè´¹ã€‚

> [!NOTE]**ASCIIï¼ŒUnicodeï¼ŒUTF-8 åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ** ---> è¿™ç¯‡[ blog ](https://www.ruanyifeng.com/blog/2007/10/ascii_unicode_and_utf-8.html)å€¼å¾—æ·±å…¥å­¦ä¹ !


### 1.3 è¯çº§åˆ†è¯ (Word-based Tokenization)

è¿™æ˜¯ä¼ ç»Ÿ NLP ä¸­å¸¸ç”¨çš„æ–¹æ³•ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŒ‰ç©ºæ ¼æˆ–æ ‡ç‚¹åˆ‡åˆ†ã€‚

```python
import regex

# GPT-2 ä½¿ç”¨çš„æ­£åˆ™æ¨¡å¼ç¤ºä¾‹
GPT2_TOKENIZER_REGEX = r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""

def word_tokenizer_demo(string):
    # ç®€å•çš„æ­£åˆ™åˆ‡åˆ†
    segments = regex.findall(r"\w+|.", string)
    return segments
```

**å±€é™æ€§ï¼š**

1. è¯æ±‡é‡ä¾ç„¶å·¨å¤§ï¼ˆç”šè‡³æ¯” Unicode å­—ç¬¦è¿˜å¤šï¼‰ã€‚
2. **UNK é—®é¢˜**ï¼šè®­ç»ƒä¸­æœªè§è¿‡çš„è¯ï¼ˆOut of Vocabularyï¼‰ä¼šè¢«æ ‡è®°ä¸ºÂ \<UNK\>ï¼Œè¿™ä¼šç ´åä¿¡æ¯çš„å®Œæ•´æ€§å¹¶å½±å“å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰çš„è®¡ç®—ã€‚

## 2. æœ€ä½³å®è·µï¼š [Byte Pair Encoding](https://en.wikipedia.org/wiki/Byte-pair_encoding) (BPE)
BPE(å­—èŠ‚å¯¹ç¼–ç )æœ€åˆç”± Philip Gage åœ¨ 1994 å¹´ä½œä¸º[æ•°æ®å‹ç¼©ç®—æ³•](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)æå‡ºï¼Œåè¢«[ç¥ç»æœºå™¨ç¿»è¯‘](https://arxiv.org/abs/1508.07909)å·¥ä½œå¼•å…¥ NLP é¢†åŸŸï¼ˆåœ¨é‚£ä¹‹å‰ï¼ŒåŸºäºè¯çš„ tokenizer æ–¹æ³•æ˜¯ä¸»æµï¼‰ï¼Œå¹¶è¢« [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) ç­‰ç°ä»£å¤§æ¨¡å‹å¹¿æ³›é‡‡ç”¨ã€‚

### 2.1 æ ¸å¿ƒæ€æƒ³
BPEçš„ åŸºç¡€æƒ³æ³•ï¼šåœ¨ä¸€ä¸ªåŸå§‹æ–‡æœ¬ä¸Šè®­ç»ƒä¸€ä¸ªåˆ†è¯å™¨æ¥ç¡®å®šä¸€ä¸ªè¯è¡¨ã€‚
BPE çš„æ ¸å¿ƒç›´è§‰æ˜¯ï¼šå¸¸è§å­—ç¬¦åºåˆ—ç”¨å•ä¸ª Token è¡¨ç¤ºï¼Œç½•è§åºåˆ—ç”¨å¤šä¸ªToken è¡¨ç¤ºã€‚å®ƒæ˜¯â€œå­—ç¬¦çº§â€å’Œâ€œè¯çº§â€çš„æŠ˜ä¸­æ–¹æ¡ˆã€‚

GPT-2çš„è®ºæ–‡ä½¿ç”¨åŸºäºè¯çš„åˆ†è¯æ–¹æ³•å°†æ–‡æœ¬æ‹†åˆ†ä¸ºåˆå§‹ç‰‡æ®µï¼Œå¹¶åœ¨æ¯ä¸ªç‰‡æ®µä¸Šè¿è¡ŒåŸå§‹çš„BPEç®—æ³•ã€‚

> [!TIP] **BPE ç®—æ³•æµç¨‹è‰å›¾**
>
> 1. **åˆå§‹åŒ–**ï¼šä»å­—èŠ‚çº§ Token å¼€å§‹ï¼ˆåˆå§‹è¯è¡¨å¤§å°ä¸º 256ï¼‰ã€‚
> 2. **ç»Ÿè®¡é¢‘ç‡**ï¼šéå†è¯­æ–™ï¼Œç»Ÿè®¡æ‰€æœ‰ç›¸é‚» Token å¯¹çš„å‡ºç°é¢‘ç‡ã€‚
> 3. **å¯»æ‰¾æœ€é¢‘å¯¹**ï¼šæ‰¾åˆ°å‡ºç°é¢‘ç‡æœ€é«˜çš„ä¸€å¯¹ï¼ˆä¾‹å¦‚ `(e, s)`ï¼‰ã€‚
> 4. **åˆå¹¶ä¸æ›´æ–°**ï¼šå°†è¿™å¯¹ Token åˆå¹¶ä¸ºä¸€ä¸ªæ–°çš„ Tokenï¼ˆä¾‹å¦‚ `es`ï¼Œåˆ†é…æ–°ç´¢å¼• 256ï¼‰ï¼Œå¹¶æ›´æ–°è¯è¡¨ã€‚
> 5. **è¿­ä»£**ï¼šé‡å¤ä¸Šè¿°æ­¥éª¤ 2-4ï¼Œç›´åˆ°è¾¾åˆ°é¢„è®¾çš„åˆå¹¶æ¬¡æ•°æˆ–ç›®æ ‡è¯è¡¨å¤§å°ã€‚

### 2.2 è®­ç»ƒä¸€ä¸ª tokenizer

```python
string = "the cat in the hat"
params = train_bpe(string, num_merges=3)

def train_bpe(string: str, num_merges: int) -> BPETokenizerParams:
	"""
	è¿™é‡Œæ˜¯ä¸€ä¸ªç®€å•çš„è®­ç»ƒ bpe çš„ä»£ç åŸå‹
	æŒ‰ç…§ä¹‹å‰çš„è®¾æƒ³ï¼Œæˆ‘ä»¬åº”è¯¥ç»´æŠ¤å¥½ä¸‹é¢ä¸‰ä¸ªå˜é‡ï¼š
	1. indices æ‰€æœ‰åŸå§‹æ–‡æ¡£çš„ utf-8 å­—èŠ‚ç¼–ç 
	2. merges è®°å½•æ¯æ¬¡éå†æ—¶çš„ç›¸é‚» token å…±ç°è®¡æ•°å™¨
	3. åˆå§‹ vocab ï¼Œæ‰€æœ‰çš„è¯è¡¨æ‹“å±•éƒ½ä» 256 ä¸ªå­—èŠ‚ç¼–ç å¼€å§‹ã€‚
	   
	"""
	indices = list(map(int, string.encode("utf-8")))
	merges: dict[tuple[int, int], int] = {} # å¯¹å…³è” index è¿›è¡Œåˆå¹¶ index1,index2 => merged index
	vocab: dict[int, bytes] =  {x: bytes([x]) for x in range(256)} # index -> bytes
	
	for i in range(num_merges):
		# ç»Ÿè®¡æ¯å¯¹ tokens çš„å‡ºç°æ¬¡æ•°
		counts = defaultdict(int)
		for index1, index2 in zip(indices, indices[1:]): # æŠ½å–æ¯ä¸€ä¸ªç›¸é‚» pair
			counts[(index1, index2)] += 1
			
		# æ‰¾åˆ°å‡ºç°æ¬¡æ•°æœ€å¤šçš„ pair
		pair = max(counts, key=counts.get)
		index1, index2 = pair
		
		# èåˆ Pair ï¼Œæ–°å¢ Token
		new_index = 256 + i
		merges[pair] = new_index
		vocab[new_index] = vocab[index1] + vocab[index2]
		indices = merge(indices, pair, new_index)
		
	return BPETokenizerParams(vocab=vocab, merges=merges)
	
def merge(indice: list[int], pair: tuple[int, int], new_index: int) -> list[int]:
	"""è¿”å›`indices`ï¼Œä½†å°†æ‰€æœ‰`pair`å®ä¾‹æ›¿æ¢ä¸º`new_index`"""
	# åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„ indice æ¥å®Œæˆå¯¹ç°æœ‰ indice çš„æ›´æ–°
	new_indices = []
	i = 0
	# 
	while i < len(indices):
		if i + 1 < len(indices) and indices[i] == pair[0] and indices[i+1] == pair[1]:
			new_indices.append(new_index)
			i += 2
		else:
			new_indices.append(indices[i])
			i += 1
	return new_indices
		
```

### 2.3 ä½¿ç”¨tokenizer

```python
# æ¨ç†é˜¶æ®µ
tokenizer = BPETokenizer(params)
test_string = "the quick brown fox"

# ç¼–ç ä¸è§£ç 
indices = tokenizer.encode(test_string)
reconstructed_string = tokenizer.decode(indices)

assert test_string == reconstructed_string
print(f"åŸå§‹å­—ç¬¦ä¸²: {test_string}")
print(f"Token ID: {indices}")


```


### 2.4 å¯äº¤äº’ python ç¯å¢ƒ
{{< py-ide >}}
```python
# è¿™æ˜¯ä¸€ä¸ªå¯ç¼–è¾‘çš„ Python ç¯å¢ƒ
from collections import defaultdict

class BPETokenizerParams:
    """All you need to specify a BPETokenizer."""
    def __init__(self, vocab, merges):
        self.vocab = vocab
        self.merges = merges

# è¾…åŠ©å‡½æ•° merge éœ€è¦å…ˆå®šä¹‰ï¼Œæˆ–è€…æ˜¯æ”¾åœ¨ç±»å¤–é¢
def merge(indices: list[int], pair: tuple[int, int], new_index: int) -> list[int]:
    new_indices = []
    i = 0
    while i < len(indices):
        # æ£€æŸ¥æ˜¯å¦åŒ¹é… pair
        if i + 1 < len(indices) and indices[i] == pair[0] and indices[i+1] == pair[1]:
            new_indices.append(new_index)
            i += 2
        else:
            new_indices.append(indices[i])
            i += 1
    return new_indices

def train_bpe(string: str, num_merges: int) -> BPETokenizerParams:
    # å°†å­—ç¬¦ä¸²è½¬ä¸º UTF-8 å­—èŠ‚æ•´æ•°åˆ—è¡¨
    indices = list(map(int, string.encode("utf-8")))
    merges = {} 
    vocab = {x: bytes([x]) for x in range(256)}
    
    for i in range(num_merges):
        # ç»Ÿè®¡æ¯å¯¹ tokens çš„å‡ºç°æ¬¡æ•°
        counts = defaultdict(int)
        
        # ã€é”™è¯¯ä¿®æ­£ 1ã€‘è¿™é‡Œä¹‹å‰å†™æˆäº† indice[1:]ï¼Œåº”è¯¥æ˜¯ indices[1:]
        for index1, index2 in zip(indices, indices[1:]): 
            counts[(index1, index2)] += 1
            
        if not counts:
            break
            
        # æ‰¾åˆ°å‡ºç°æ¬¡æ•°æœ€å¤šçš„ pair
        pair = max(counts, key=counts.get)
        index1, index2 = pair
        
        # èåˆ Pair ï¼Œæ–°å¢ Token
        # ã€é”™è¯¯ä¿®æ­£ 2ã€‘è¿™é‡Œä¹‹å‰å®šä¹‰äº† new_pairï¼Œä½†åé¢ç”¨çš„æ˜¯ new_index
        new_index = 256 + i
        
        merges[pair] = new_index
        vocab[new_index] = vocab[index1] + vocab[index2]
        
        # è°ƒç”¨ merge æ›´æ–° indices
        indices = merge(indices, pair, new_index)
        
    return BPETokenizerParams(vocab=vocab, merges=merges)

# æµ‹è¯•è¿è¡Œ
string = "the cat in the hat"
params = train_bpe(string, num_merges=3)

print("Training complete!")
print("Merges:", params.merges)
print("Vocab size:", len(params.vocab))

```
{{< /py-ide >}}


## æ€»ç»“

åˆ†è¯æ˜¯ä¸€ç§â€œå¿…è¦çš„æ¶ï¼ˆNecessary Evilï¼‰â€ã€‚è™½ç„¶ç†æƒ³æƒ…å†µä¸‹æˆ‘ä»¬å¸Œæœ›æ¨¡å‹èƒ½ç›´æ¥ç†è§£åŸå§‹å­—èŠ‚ï¼Œä½† BPE æ˜¯ç›®å‰å¹³è¡¡è¯è¡¨å¤§å°ã€åºåˆ—é•¿åº¦å’Œä¿¡æ¯å¯†åº¦çš„æœ€æœ‰æ•ˆæ‰‹æ®µã€‚

åœ¨å®é™…çš„å¤§æ¨¡å‹è®­ç»ƒï¼ˆå¦‚ GPT-2/3/4ï¼‰ä¸­ï¼ŒBPE çš„å®ç°ä¼šæ›´åŠ å¤æ‚ï¼ŒåŒ…æ‹¬ï¼š

1. **é¢„åˆ†è¯ï¼ˆPre-tokenizationï¼‰**ï¼šå…ˆç”¨æ­£åˆ™å°†æ–‡æœ¬åˆ‡åˆ†ä¸ºå•è¯å—ï¼Œå†åœ¨å—å†…è¿›è¡Œ BPEï¼Œé˜²æ­¢è·¨å•è¯åˆå¹¶ã€‚
2. **ç‰¹æ®Š Token**ï¼šå¤„ç†Â <|endoftext|>Â ç­‰æ§åˆ¶ç¬¦ã€‚
3. **æ€§èƒ½ä¼˜åŒ–**ï¼šç¼–ç è¿‡ç¨‹éœ€è¦æåº¦ä¼˜åŒ–ï¼Œä¸èƒ½åƒä¸Šè¿°ä»£ç é‚£æ ·éå†æ‰€æœ‰åˆå¹¶è§„åˆ™ã€‚